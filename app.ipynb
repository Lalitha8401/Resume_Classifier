{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Category                                             Resume\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n",
      "Category\n",
      "Java Developer               84\n",
      "Testing                      70\n",
      "DevOps Engineer              55\n",
      "Python Developer             48\n",
      "Web Designing                45\n",
      "HR                           44\n",
      "Hadoop                       42\n",
      "Sales                        40\n",
      "Data Science                 40\n",
      "Mechanical Engineer          40\n",
      "ETL Developer                40\n",
      "Blockchain                   40\n",
      "Operations Manager           40\n",
      "Arts                         36\n",
      "Database                     33\n",
      "Health and fitness           30\n",
      "PMO                          30\n",
      "Electrical Engineering       30\n",
      "Business Analyst             28\n",
      "DotNet Developer             28\n",
      "Automation Testing           26\n",
      "Network Security Engineer    25\n",
      "Civil Engineer               24\n",
      "SAP Developer                24\n",
      "Advocate                     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (Replace 'resume_dataset.csv' with your actual filename)\n",
    "df = pd.read_csv(\"UpdatedResumeDataSet.csv\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(df.head())\n",
    "print(df['Category'].value_counts())  # Check category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Lalitha Raja/nltk_data', 'd:\\\\Projects\\\\Resume1\\\\venv\\\\nltk_data', 'd:\\\\Projects\\\\Resume1\\\\venv\\\\share\\\\nltk_data', 'd:\\\\Projects\\\\Resume1\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Lalitha Raja\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.3\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "print(fitz.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lalitha R \n",
      "PG - DATA SCIENCE STUDENT \n",
      "+91 7812843451\n",
      "lalitha08042001@gmail.com\n",
      "https://github.com/Lalitha8401\n",
      "linkedin.com/in/lalitha-r-1554322a1/\n",
      "PROFILE \n",
      "Dynamic and detail-oriented data enthusiast with the knowledge in statistical analysis, machine learning, \n",
      "big data analytics and programming languages such as Python, SQL and NOSQL. Data-driven problem \n",
      "solver with a passion for extracting insights and driving impactful decisions and eager to contribute to \n",
      "dynamic projects in a data-centric\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"Lalitha.R_resume.pdf\"  # Replace with an actual resume\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "print(text[:500])  # Print the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]  # Lemmatization\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"processed_text\"] = df[\"Resume\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 words\n",
    "X = vectorizer.fit_transform(df[\"processed_text\"])\n",
    "y = df[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(4106))\t0.015337213683749325\n",
      "  (np.int32(0), np.int32(3481))\t0.020572323157520212\n",
      "  (np.int32(0), np.int32(2502))\t0.04999072605065765\n",
      "  (np.int32(0), np.int32(3554))\t0.1239509291151046\n",
      "  (np.int32(0), np.int32(3209))\t0.037005099843467365\n",
      "  (np.int32(0), np.int32(3044))\t0.031160547425873823\n",
      "  (np.int32(0), np.int32(3938))\t0.04350269401273844\n",
      "  (np.int32(0), np.int32(3937))\t0.12204827174382772\n",
      "  (np.int32(0), np.int32(2528))\t0.07885019885414829\n",
      "  (np.int32(0), np.int32(2744))\t0.08700538802547687\n",
      "  (np.int32(0), np.int32(4217))\t0.0166122801299208\n",
      "  (np.int32(0), np.int32(2397))\t0.017307472838667612\n",
      "  (np.int32(0), np.int32(2399))\t0.08730472361884616\n",
      "  (np.int32(0), np.int32(2422))\t0.09559572470659033\n",
      "  (np.int32(0), np.int32(2641))\t0.023965326455836815\n",
      "  (np.int32(0), np.int32(2531))\t0.04640835841019379\n",
      "  (np.int32(0), np.int32(3690))\t0.028223530419837668\n",
      "  (np.int32(0), np.int32(4362))\t0.04350269401273844\n",
      "  (np.int32(0), np.int32(2969))\t0.08392765868910729\n",
      "  (np.int32(0), np.int32(608))\t0.08392765868910729\n",
      "  (np.int32(0), np.int32(2465))\t0.04350269401273844\n",
      "  (np.int32(0), np.int32(3604))\t0.03862554620491405\n",
      "  (np.int32(0), np.int32(1853))\t0.041963829344553644\n",
      "  (np.int32(0), np.int32(1268))\t0.054486453104914116\n",
      "  (np.int32(0), np.int32(4621))\t0.03566805226590491\n",
      "  :\t:\n",
      "  (np.int32(961), np.int32(1459))\t0.17548005877564316\n",
      "  (np.int32(961), np.int32(1733))\t0.05767132226881625\n",
      "  (np.int32(961), np.int32(1127))\t0.048414378813548715\n",
      "  (np.int32(961), np.int32(507))\t0.05538321786102684\n",
      "  (np.int32(961), np.int32(2423))\t0.10461007325640524\n",
      "  (np.int32(961), np.int32(3412))\t0.12018807013866363\n",
      "  (np.int32(961), np.int32(4469))\t0.1827845923144974\n",
      "  (np.int32(961), np.int32(2138))\t0.0647555224316019\n",
      "  (np.int32(961), np.int32(865))\t0.06330307399459485\n",
      "  (np.int32(961), np.int32(4581))\t0.13232356521110855\n",
      "  (np.int32(961), np.int32(386))\t0.17690849380720683\n",
      "  (np.int32(961), np.int32(1812))\t0.055921775822794846\n",
      "  (np.int32(961), np.int32(3917))\t0.05486424822388238\n",
      "  (np.int32(961), np.int32(2755))\t0.06695526685275939\n",
      "  (np.int32(961), np.int32(4838))\t0.07187617240283234\n",
      "  (np.int32(961), np.int32(3396))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(2659))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(2758))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(3884))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(3217))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(706))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(4339))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(4875))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(4387))\t0.07808820143670728\n",
      "  (np.int32(961), np.int32(4767))\t0.07808820143670728\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Data Science\n",
      "1      Data Science\n",
      "2      Data Science\n",
      "3      Data Science\n",
      "4      Data Science\n",
      "           ...     \n",
      "957         Testing\n",
      "958         Testing\n",
      "959         Testing\n",
      "960         Testing\n",
      "961         Testing\n",
      "Name: Category, Length: 962, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 99.48%\n",
      "SVM Accuracy: 99.48%\n",
      "Random Forest Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"SVM\": SVC(kernel=\"linear\"),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1}\n",
      "Final Optimized SVM Accuracy: 99.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"C\": [0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(SVC(kernel=\"linear\"), param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Final Accuracy\n",
    "y_pred = best_svm.predict(X_test)\n",
    "print(f\"Final Optimized SVM Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(best_svm, \"resume_classifier_model.pkl\")\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Resume Classification Result</h2><p>The resume has been categorized as: <b>Data Science</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import fitz  # PyMuPDF\n",
    "from IPython.display import display, HTML  # Import for displaying HTML\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ... (rest of your code for loading model, vectorizer, and preprocessing) ...\n",
    "\n",
    "\n",
    "def classify_resume_notebook(file_path):\n",
    "    \"\"\"\n",
    "    Classifies a resume and displays the result in the notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the resume PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        text_vector = vectorizer.transform([processed_text])\n",
    "        category = model.predict(text_vector)[0]\n",
    "\n",
    "        # Display the result in the notebook\n",
    "        display(HTML(f\"<h2>Resume Classification Result</h2><p>The resume has been categorized as: <b>{category}</b></p>\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        display(HTML(f\"<p style='color:red;'>Error: {e}</p>\"))\n",
    "\n",
    "\n",
    "# Example usage within the notebook:\n",
    "resume_file_path = \"Lalitha.R_resume.pdf\"  # Replace with your actual resume path\n",
    "classify_resume_notebook(resume_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lalitha\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "model = joblib.load(\"resume_classifier_model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Function to classify resume\n",
    "def classify_resume():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
    "    if not file_path:\n",
    "        return\n",
    "    \n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    processed_text = preprocess_text(text)\n",
    "    text_vector = vectorizer.transform([processed_text])\n",
    "    \n",
    "    category = model.predict(text_vector)[0]\n",
    "    \n",
    "    # Save resume to categorized folder\n",
    "    folder_path = os.path.join(\"Classified_Resumes\", category)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    shutil.copy(file_path, os.path.join(folder_path, os.path.basename(file_path)))\n",
    "\n",
    "    messagebox.showinfo(\"Success\", f\"Resume categorized as '{category}' and saved in '{folder_path}'.\")\n",
    "\n",
    "# GUI Design\n",
    "root = tk.Tk()\n",
    "root.title(\"Resume Classifier\")\n",
    "root.geometry(\"400x300\")\n",
    "\n",
    "label = tk.Label(root, text=\"Upload a Resume for Classification\", font=(\"Arial\", 12))\n",
    "label.pack(pady=20)\n",
    "\n",
    "upload_btn = tk.Button(root, text=\"Upload Resume\", command=classify_resume, font=(\"Arial\", 10), bg=\"lightblue\")\n",
    "upload_btn.pack(pady=10)\n",
    "\n",
    "exit_btn = tk.Button(root, text=\"Exit\", command=root.quit, font=(\"Arial\", 10), bg=\"red\", fg=\"white\")\n",
    "exit_btn.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
